{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Data Processing Pipeline\n",
    "## Processing Combined Multi-Crypto Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.8f' % x)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined cryptocurrency data\n",
    "df = pd.read_csv('crypto_data_combined.csv')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET LOADED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique cryptocurrencies: {df['Symbol'].nunique()}\")\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info and missing values\n",
    "print(\"\\nDataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\" * 70)\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"✅ No missing values found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CRYPTOCURRENCY DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(df['Symbol'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Handle missing values (if any)\n",
    "initial_rows = len(df)\n",
    "df = df.dropna(subset=['Close', 'Volume', 'Open', 'High', 'Low'])\n",
    "dropped_rows = initial_rows - len(df)\n",
    "\n",
    "# Remove duplicates\n",
    "initial_rows = len(df)\n",
    "df = df.drop_duplicates(subset=['Symbol', 'Date'])\n",
    "duplicate_rows = initial_rows - len(df)\n",
    "\n",
    "# Sort by Symbol and Date\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Remove any rows with zero or negative prices\n",
    "df = df[(df['Close'] > 0) & (df['Open'] > 0) & (df['High'] > 0) & (df['Low'] > 0)]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Rows dropped (missing values): {dropped_rows}\")\n",
    "print(f\"Duplicate rows removed: {duplicate_rows}\")\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "print(f\"Total trading days: {df['Date'].nunique()}\")\n",
    "print(f\"Cryptocurrencies: {df['Symbol'].nunique()}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering - Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create technical indicators and features\n",
    "def add_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    Add technical indicators to the dataframe\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Price features\n",
    "    df_copy['Price_Change'] = df_copy['Close'] - df_copy['Open']\n",
    "    df_copy['Price_Change_Pct'] = (df_copy['Price_Change'] / df_copy['Open']) * 100\n",
    "    df_copy['Daily_Range'] = df_copy['High'] - df_copy['Low']\n",
    "    df_copy['Volatility'] = (df_copy['Daily_Range'] / df_copy['High']) * 100\n",
    "    \n",
    "    # Average price\n",
    "    df_copy['Avg_Price'] = (df_copy['High'] + df_copy['Low'] + df_copy['Close']) / 3\n",
    "    \n",
    "    # Typical Price (HLC/3)\n",
    "    df_copy['Typical_Price'] = (df_copy['High'] + df_copy['Low'] + df_copy['Close']) / 3\n",
    "    \n",
    "    # Body size (candle)\n",
    "    df_copy['Body_Size'] = abs(df_copy['Close'] - df_copy['Open'])\n",
    "    df_copy['Body_Size_Pct'] = (df_copy['Body_Size'] / df_copy['Open']) * 100\n",
    "    \n",
    "    # Upper and Lower shadows\n",
    "    df_copy['Upper_Shadow'] = df_copy['High'] - df_copy[['Open', 'Close']].max(axis=1)\n",
    "    df_copy['Lower_Shadow'] = df_copy[['Open', 'Close']].min(axis=1) - df_copy['Low']\n",
    "    \n",
    "    # Bullish/Bearish indicator\n",
    "    df_copy['Is_Bullish'] = (df_copy['Close'] > df_copy['Open']).astype(int)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "print(\"Adding technical indicators...\")\n",
    "df_processed = add_technical_indicators(df)\n",
    "print(f\"✅ Technical indicators added! New shape: {df_processed.shape}\")\n",
    "print(f\"New columns: {df_processed.shape[1] - df.shape[1]}\")\n",
    "\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add moving averages and rolling statistics\n",
    "def add_rolling_features(df, windows=[7, 14, 21, 30]):\n",
    "    \"\"\"\n",
    "    Add moving averages and rolling statistics\n",
    "    Calculated per cryptocurrency (grouped by Symbol)\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    print(f\"Calculating rolling features for windows: {windows}\")\n",
    "    \n",
    "    for window in windows:\n",
    "        print(f\"  - Processing {window}-day window...\")\n",
    "        \n",
    "        # Moving averages (SMA)\n",
    "        df_copy[f'SMA_{window}'] = df_copy.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Exponential Moving Average (EMA)\n",
    "        df_copy[f'EMA_{window}'] = df_copy.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.ewm(span=window, adjust=False).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling volatility (standard deviation)\n",
    "        df_copy[f'Rolling_Std_{window}'] = df_copy.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "        \n",
    "        # Rolling volume average\n",
    "        df_copy[f'Volume_MA_{window}'] = df_copy.groupby('Symbol')['Volume'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling min and max\n",
    "        df_copy[f'Rolling_Max_{window}'] = df_copy.groupby('Symbol')['High'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        df_copy[f'Rolling_Min_{window}'] = df_copy.groupby('Symbol')['Low'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "df_processed = add_rolling_features(df_processed)\n",
    "print(f\"\\n✅ Rolling features added! New shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering - Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lag features (previous day values)\n",
    "def add_lag_features(df, lags=[1, 2, 3, 5, 7]):\n",
    "    \"\"\"\n",
    "    Add lag features (previous values)\n",
    "    Grouped by Symbol to avoid mixing different cryptocurrencies\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    print(f\"Adding lag features for periods: {lags}\")\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(f\"  - Creating {lag}-day lag features...\")\n",
    "        \n",
    "        # Lag close prices\n",
    "        df_copy[f'Close_Lag_{lag}'] = df_copy.groupby('Symbol')['Close'].shift(lag)\n",
    "        \n",
    "        # Lag volume\n",
    "        df_copy[f'Volume_Lag_{lag}'] = df_copy.groupby('Symbol')['Volume'].shift(lag)\n",
    "        \n",
    "        # Lag returns\n",
    "        df_copy[f'Return_Lag_{lag}'] = df_copy.groupby('Symbol')['Price_Change_Pct'].shift(lag)\n",
    "        \n",
    "        # Lag volatility\n",
    "        df_copy[f'Volatility_Lag_{lag}'] = df_copy.groupby('Symbol')['Volatility'].shift(lag)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "df_processed = add_lag_features(df_processed)\n",
    "print(f\"\\n✅ Lag features added! New shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering - Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time-based features\n",
    "print(\"Adding time-based features...\")\n",
    "\n",
    "df_processed['Year'] = df_processed['Date'].dt.year\n",
    "df_processed['Month'] = df_processed['Date'].dt.month\n",
    "df_processed['Day'] = df_processed['Date'].dt.day\n",
    "df_processed['DayOfWeek'] = df_processed['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df_processed['Quarter'] = df_processed['Date'].dt.quarter\n",
    "df_processed['DayOfYear'] = df_processed['Date'].dt.dayofyear\n",
    "df_processed['WeekOfYear'] = df_processed['Date'].dt.isocalendar().week\n",
    "\n",
    "# Is weekend?\n",
    "df_processed['Is_Weekend'] = (df_processed['DayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "# Month start/end\n",
    "df_processed['Is_Month_Start'] = df_processed['Date'].dt.is_month_start.astype(int)\n",
    "df_processed['Is_Month_End'] = df_processed['Date'].dt.is_month_end.astype(int)\n",
    "\n",
    "print(f\"✅ Time-based features added! Final shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Target Variable - Next Day Close Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable: Next day's closing price\n",
    "print(\"Creating target variable (Next Day Close Price)...\")\n",
    "\n",
    "df_processed['Next_Day_Close'] = df_processed.groupby('Symbol')['Close'].shift(-1)\n",
    "\n",
    "# Also create target for price change\n",
    "df_processed['Next_Day_Price_Change'] = df_processed['Next_Day_Close'] - df_processed['Close']\n",
    "df_processed['Next_Day_Price_Change_Pct'] = (\n",
    "    (df_processed['Next_Day_Close'] - df_processed['Close']) / df_processed['Close']\n",
    ") * 100\n",
    "\n",
    "# Binary classification target (Up/Down)\n",
    "df_processed['Next_Day_Direction'] = (df_processed['Next_Day_Close'] > df_processed['Close']).astype(int)\n",
    "\n",
    "print(\"✅ Target variables created!\")\n",
    "print(\"\\nTarget variables:\")\n",
    "print(\"  - Next_Day_Close: Actual next day closing price (Regression)\")\n",
    "print(\"  - Next_Day_Price_Change_Pct: Percentage change (Regression)\")\n",
    "print(\"  - Next_Day_Direction: 1=Up, 0=Down (Classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL PROCESSED DATASET SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total records: {len(df_processed):,}\")\n",
    "print(f\"Total features: {df_processed.shape[1]}\")\n",
    "print(f\"Cryptocurrencies: {df_processed['Symbol'].nunique()}\")\n",
    "print(f\"Date range: {df_processed['Date'].min().date()} to {df_processed['Date'].max().date()}\")\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing = df_processed.isnull().sum()\n",
    "missing_pct = (missing / len(df_processed)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing[missing > 0],\n",
    "    'Missing_Pct': missing_pct[missing > 0]\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"✅ No missing values!\")\n",
    "\n",
    "print(\"\\nFeature categories:\")\n",
    "feature_cols = [col for col in df_processed.columns if col not in ['SNo', 'Name', 'Symbol', 'Date']]\n",
    "print(f\"  - Total features: {len(feature_cols)}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset\n",
    "output_file = 'crypto_data_processed.csv'\n",
    "df_processed.to_csv(output_file, index=False)\n",
    "print(f\"✅ Processed data saved to: {output_file}\")\n",
    "\n",
    "# Save statistics per coin\n",
    "print(\"\\nGenerating coin-wise statistics...\")\n",
    "coin_stats = df_processed.groupby('Symbol').agg({\n",
    "    'Name': 'first',\n",
    "    'Close': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'Volume': ['mean', 'sum'],\n",
    "    'Volatility': 'mean',\n",
    "    'Price_Change_Pct': ['mean', 'std'],\n",
    "    'Date': ['min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "coin_stats.columns = ['_'.join(str(col)).strip('_') for col in coin_stats.columns.values]\n",
    "coin_stats.to_csv('coin_statistics.csv', index=False)\n",
    "print(\"✅ Coin statistics saved to: coin_statistics.csv\")\n",
    "\n",
    "# Save feature list\n",
    "feature_list = pd.DataFrame({\n",
    "    'Feature_Name': df_processed.columns.tolist(),\n",
    "    'Data_Type': df_processed.dtypes.values.astype(str)\n",
    "})\n",
    "feature_list.to_csv('feature_list.csv', index=False)\n",
    "print(\"✅ Feature list saved to: feature_list.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. crypto_data_processed.csv - Full processed dataset\")\n",
    "print(\"  2. coin_statistics.csv - Summary stats per cryptocurrency\")\n",
    "print(\"  3. feature_list.csv - List of all features\")\n",
    "print(\"\\nNext step: Run 02_exploratory_data_analysis.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of processed data\n",
    "print(\"\\nSample of processed data:\")\n",
    "df_processed[['Date', 'Symbol', 'Close', 'Next_Day_Close', 'Next_Day_Direction', \n",
    "              'SMA_7', 'SMA_14', 'Volatility', 'Price_Change_Pct']].head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
