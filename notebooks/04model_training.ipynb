{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Price Prediction - ML Models\n",
    "## Multiple ML Algorithms for Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "df = pd.read_csv('crypto_data_processed.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_cols = [\n",
    "    'Open', 'High', 'Low', 'Volume', 'Marketcap',\n",
    "    'Price_Change', 'Daily_Range', 'Volatility', 'Avg_Price',\n",
    "    'Body_Size', 'Upper_Shadow', 'Lower_Shadow',\n",
    "    'MA_7', 'MA_14', 'MA_30',\n",
    "    'Volatility_7d', 'Volatility_14d', 'Volatility_30d',\n",
    "    'Volume_MA_7', 'Volume_MA_14', 'Volume_MA_30',\n",
    "    'Close_Lag_1', 'Close_Lag_3', 'Close_Lag_7',\n",
    "    'Volume_Lag_1', 'Volume_Lag_3', 'Volume_Lag_7',\n",
    "    'Return_Lag_1', 'Return_Lag_3', 'Return_Lag_7',\n",
    "    'Year', 'Month', 'Day', 'DayOfWeek', 'Quarter', 'DayOfYear'\n",
    "]\n",
    "\n",
    "# Filter only existing columns\n",
    "feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "target_col = 'Close'\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_model = df[feature_cols + [target_col, 'Symbol']].dropna()\n",
    "\n",
    "print(f\"Model dataset shape: {df_model.shape}\")\n",
    "print(f\"Features used: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature list: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[target_col]\n",
    "\n",
    "# Split data (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Train mean: ${y_train.mean():,.2f}\")\n",
    "print(f\"  Test mean: ${y_test.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "print(\"Feature scaling completed and scaler saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Train_MAE': train_mae,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2\n",
    "    }\n",
    "    \n",
    "    return results, model, y_test_pred\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models and results\n",
    "models_dict = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=1.0),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, max_depth=7, learning_rate=0.1, random_state=42),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, max_depth=7, learning_rate=0.1, random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "trained_models = {}\n",
    "predictions = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for linear models, original for tree-based\n",
    "    if name in ['Linear Regression', 'Ridge', 'Lasso', 'SVR']:\n",
    "        results, trained_model, y_pred = evaluate_model(\n",
    "            model, X_train_scaled, X_test_scaled, y_train, y_test, name\n",
    "        )\n",
    "    else:\n",
    "        results, trained_model, y_pred = evaluate_model(\n",
    "            model, X_train, X_test, y_train, y_test, name\n",
    "        )\n",
    "    \n",
    "    all_results.append(results)\n",
    "    trained_models[name] = trained_model\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    print(f\"  Test RMSE: ${results['Test_RMSE']:,.2f}, Test R²: {results['Test_R2']:.4f}\\n\")\n",
    "\n",
    "print(\"All models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('models/model_comparison.csv', index=False)\n",
    "print(\"\\nResults saved to 'models/model_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# RMSE Comparison\n",
    "x_pos = np.arange(len(results_df))\n",
    "axes[0, 0].barh(x_pos, results_df['Test_RMSE'], color='steelblue', alpha=0.7, label='Test')\n",
    "axes[0, 0].barh(x_pos, results_df['Train_RMSE'], color='lightcoral', alpha=0.5, label='Train')\n",
    "axes[0, 0].set_yticks(x_pos)\n",
    "axes[0, 0].set_yticklabels(results_df['Model'])\n",
    "axes[0, 0].set_xlabel('RMSE')\n",
    "axes[0, 0].set_title('Model RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# R² Comparison\n",
    "axes[0, 1].barh(x_pos, results_df['Test_R2'], color='darkgreen', alpha=0.7, label='Test')\n",
    "axes[0, 1].barh(x_pos, results_df['Train_R2'], color='lightgreen', alpha=0.5, label='Train')\n",
    "axes[0, 1].set_yticks(x_pos)\n",
    "axes[0, 1].set_yticklabels(results_df['Model'])\n",
    "axes[0, 1].set_xlabel('R² Score')\n",
    "axes[0, 1].set_title('Model R² Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# MAE Comparison\n",
    "axes[1, 0].barh(x_pos, results_df['Test_MAE'], color='darkorange', alpha=0.7, label='Test')\n",
    "axes[1, 0].barh(x_pos, results_df['Train_MAE'], color='gold', alpha=0.5, label='Train')\n",
    "axes[1, 0].set_yticks(x_pos)\n",
    "axes[1, 0].set_yticklabels(results_df['Model'])\n",
    "axes[1, 0].set_xlabel('MAE')\n",
    "axes[1, 0].set_title('Model MAE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Best model prediction vs actual\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_predictions = predictions[best_model_name]\n",
    "sample_size = min(1000, len(y_test))\n",
    "sample_idx = np.random.choice(len(y_test), sample_size, replace=False)\n",
    "\n",
    "axes[1, 1].scatter(y_test.iloc[sample_idx], best_predictions[sample_idx], \n",
    "                   alpha=0.5, s=20, color='purple')\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1, 1].set_xlabel('Actual Price')\n",
    "axes[1, 1].set_ylabel('Predicted Price')\n",
    "axes[1, 1].set_title(f'Best Model: {best_model_name}\\nPrediction vs Actual', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for best tree-based model\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importance.head(20)['Feature'], \n",
    "             feature_importance.head(20)['Importance'],\n",
    "             color='teal')\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.title(f'Top 20 Feature Importance - {best_model_name}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv('models/feature_importance.csv', index=False)\n",
    "else:\n",
    "    print(f\"{best_model_name} does not provide feature importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model_path = f'models/best_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl'\n",
    "joblib.dump(best_model, best_model_path)\n",
    "\n",
    "# Save all models\n",
    "for name, model in trained_models.items():\n",
    "    model_path = f'models/{name.replace(\" \", \"_\").lower()}_model.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "print(f\"\\nBest model ({best_model_name}) saved to: {best_model_path}\")\n",
    "print(f\"All models saved to 'models/' directory\")\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "print(f\"  Test R²: {results_df.iloc[0]['Test_R2']:.4f}\")\n",
    "print(f\"  Test RMSE: ${results_df.iloc[0]['Test_RMSE']:,.2f}\")\n",
    "print(f\"  Test MAE: ${results_df.iloc[0]['Test_MAE']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for best model\n",
    "residuals = y_test - best_predictions\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Residuals distribution\n",
    "axes[0, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='navy')\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Residuals')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Residuals Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0, 1].scatter(best_predictions, residuals, alpha=0.5, s=20, color='darkgreen')\n",
    "axes[0, 1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residuals vs Predicted', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Residuals over index\n",
    "axes[1, 1].scatter(range(len(residuals)), residuals, alpha=0.5, s=20, color='purple')\n",
    "axes[1, 1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Index')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].set_title('Residuals Over Index', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Residual Analysis - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean: {residuals.mean():.6f}\")\n",
    "print(f\"  Std: {residuals.std():.6f}\")\n",
    "print(f\"  Min: {residuals.min():.6f}\")\n",
    "print(f\"  Max: {residuals.max():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
